---
title: "jobsta"
subtitle: |
  | _Study - Job - Money_
  | 
  | Project for
  | Introduction to Semantic Systems
  | (188.399-2019W)
author: |
  | Group 01
  |
  | Cem Bicer (01425692)
  | Helmuth Breitenfellner (08725866)
  | Laszlo Kiraly (09227679)
  | Gerald Weber (00125536)
date: "2020-01-31"
output:
  beamer_presentation:
    theme: "Pittsburgh"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## App Idea

```{r logo, echo=FALSE, out.width="30%"}
knitr::include_graphics("jobsta-logo.png")
```

_Study - Job - Money_

* For Software Developers and Data Scientists
* Asks for experience, age, location
* Answers to following questions:
    * _What shall I study?_
    * _Where shall I work?_
    * _What shall I practise?_
    * _How can I improve?_

## The Mobile App

```{r fig.align="center", out.width="35%"}
knitr::include_graphics('iphone-mockup.png')
```

## Data Sources

* **Kaggle User Survey**$\hfill\break$
  Data Scientists, Country, Job Role, Programming Language, Income
* **StackOverflow User Survey**$\hfill\break$
  Software Developer, Country, Job Role, Programming Language, Income
* **GitHub Repositories Data**$\hfill\break$
  Repository URL, Popularity, Programming Language, Issues
* **TISS Lectures**$\hfill\break$
  Lectures, Lecturer, Description, Programming Language

## Kaggle Survey

* https://www.kaggle.com/c/kaggle-survey-2019
* Used Jupyter Notebook for Pre-Processing
* Created RDF XML directly from Python
* **Challenge:** high number of one-hot-encoded values, had to extract unique values

## StackOverflow Survey

* https://insights.stackoverflow.com/survey/2018
* Used Python for Pre-Processing
* Created instances directly from Python
* **Challenge:** Excel, Numbers and TextMate could all not open the csv file (>90.000 entries) properly

## GitHub Repositories Data

* http://ghtorrent.org/
* Used Bash & R Script for Pre-Processing
* Created RDF XML directly from Python
* **Challenge:** _huge_ data archive (>100GB) had to be filtered / preprocessed

## TISS Lectures

* https://tiss.tuwien.ac.at/course/courseList.xhtml?dswid=6403&dsrid=238
* Used Python Script
* Created RDF XML directly from Python (using `rdflib`)
* **Challenge:** web scraping, identifying the programming language from text

## Ontology #1

```{r ontology, echo=FALSE, out.width="100%"}
knitr::include_graphics("ontology_new.png")
```

## Ontology #2

* Created with Protégé
* Reusing existing Ontologies
  * schema.org
  * dbpedia.org
* Entites:
  - Developer
  - dbpedia:Country
  - schema:Course
  - GitHubRepository
  - schema:ComputerLanguage
  - ...
* Object properties:
  - dealsWith
  - developsIn
  - schema:homeLocation
  - ...

## Harmonize Data I

* Age Ranges
    * Different Age Ranges

* Salary vs. Salary Range
    * Salary Range in Kaggle
    * Salary Value in Stackoverflow

* Roles
  * Combined from Surveys into List
  * e.g. Frontend Developer -> Software Engineer
  * ... C-Suite Executive -> Manager

## Harmonize Data II

* Countries
    * dbpedia linked to external data

* Gender
    * Single Selection in Kaggle
    * Multiple Selections in Stackoverflow

* Computer Language
    * Combined from Surveys into List
    * Field in Github Repository
    * Extracted from TISS Lecture Description

## SPARQL Queries #1

"As a developer I live in (Austria) and I want more than (150000 USD per year). What courses at TU Wien deal with programming languages which high-earners are using?"

## SPARQL Queries #1 (cont.)

```{r,out.width="100%"}
knitr::include_graphics('sparql-query1.png')
```

## SPARQL Queries #2

"As a developer I live in (Austria) and I can program in (Python) and I want more than (55000 USD per year). Should I stay or should I go?"


## SPARQL Queries #2 (cont.)

```{r,out.width="90%"}
knitr::include_graphics('sparql-query2.png')
```

## Lessons Learned

* Iterative process to come up with final idea

* Scraping TISS: no ID access to fields

* http://schema.org not equal to http://www.schema.org

* GraphQL Github API vs. Database Dump

* Harmonizing data can be tedious


## Questions?

Thank you for your attention!